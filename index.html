---
layout: about
title:
permalink: /
---

<p>I am a final year CS PhD student at <a href="http://epfl.ch/">EPFL</a> working with <a href="https://theory.epfl.ch/kapralov/">Michael Kapralov</a>. I am broadly interested in LLM inference and fine-tuning optimization with recent works on fast attention, KV cache compression and data-selection for LLM distillation. In the past I have also worked on fast algorithms for large-scale and high-dimensional data analysis and numerical linear algebra.</p>

<p>In my current research, I am exploring:</p>
<ul>
  <li>Data selection for distillation based targeted instruction tuning for LLMs</li>
  <li>Writing efficient SIMD kernels for speeding up graph-based retrieval algorithms via quantization</li>
  <li>Fast attention at prefill, KV cache compression, and long-context inference</li>
</ul>

<p>Experience:</p>
<ul>
  <li>Student Researcher, Google Research (October 2025-Present): Working on quantization for vector databases for graph-based retrieval algorithms and data selection for LLM fine-tuning.</li>
  <li>Applied Science Intern, Amazon Research (July 2024-Jan 2025): Deployed ML and optimization solutions to production on AWS cloud infra stack for internal customers.</li>
  <li>Research Assistant (January 2017-August 2018): Worked with Anirban Dasgupta and Dinesh Garg (IBM Research, Bengaluru) on randomized linear algebra.</li>
  <li>Caltech - SURF Fellow (May 2017-July 2017): Worked with Ashish Mahabal on deep learning for astronomy.</li>
</ul>

<div class="news">
  <h2 id="news">News</h2>
  <ul>
    <li><em>October 2025</em>: Started as a Student Researcher at Google Research.</li>
    <li><em>September 2025</em>: Paper on BalanceKV, a novel KV cache compression method, accepted to NeurIPS 2025 as Spotlight.</li>
    <li><em>January 2025</em>: Paper accepted to ICLR 2025 (first author).</li>
    <li><em>July 2024</em>: “Improved Algorithms for Kernel Matrix-Vector Multiplication” won Best Paper at the ICML 2024 Workshop on Long Context Foundation Models.</li>
    <li><em>July 2024</em>: Started as an Applied Science Intern at Amazon Research.</li>
  </ul>
</div>

<div class="news">
  <h2 id="publications">Publications</h2>
  <p>Full list also on <a href="https://scholar.google.com/citations?user=LpgWaeUAAAAJ&hl=en">Google Scholar</a>.</p>

  <p><span style="font-family: sans-serif"><strong>Streaming Attention Approximation via Discrepancy Theory</strong></span>.<br />
  Ekaterina Kochetkova, <ins>Kshiteej Sheth</ins>, Insu Han, Amir Zandieh, Michael Kapralov.<br />
  <em>NeurIPS 2025 (Spotlight)</em>.<br />
  [<a href="https://arxiv.org/abs/2502.07861"><code class="language-plaintext highlighter-rouge">arXiv</code></a> | <a href="https://github.com/ksheth96/BalanceKV"><code class="language-plaintext highlighter-rouge">Code</code></a>]</p>

  <p><span style="font-family: sans-serif"><strong>Improved Algorithms for Kernel Matrix-Vector Multiplication</strong></span>.<br />
  (alphabetical) Piotr Indyk, Michael Kapralov, <ins>Kshiteej Sheth</ins>, Tal Wagner.<br />
  <em>ICLR 2025 (Poster)</em> (I was first author). Best Paper at ICML 2024 Workshop on Long Context Foundation Models.<br />
  [<a href="https://openreview.net/forum?id=7CCzyEtZXH"><code class="language-plaintext highlighter-rouge">OpenReview</code></a> | <a href="https://longcontextfm.github.io/"><code class="language-plaintext highlighter-rouge">Workshop</code></a>]</p>

  <p><span style="font-family: sans-serif"><strong>Sublinear Time Low-Rank Approximation of Hankel Matrices</strong></span>.<br />
  Michael Kapralov, Cameron Musco, <ins>Kshiteej Sheth</ins>.<br />
  <em>SODA 2026</em>.<br />
  [<a href="https://arxiv.org/abs/2511.21418"><code class="language-plaintext highlighter-rouge">arXiv</code></a>]</p>

  <p><span style="font-family: sans-serif"><strong>Sublinear Time Low-Rank Approximation of Toeplitz Matrices</strong></span>.<br />
  Cameron Musco, <ins>Kshiteej Sheth</ins>.<br />
  <em>SODA 2024</em>.<br />
  [<a href="http://arxiv.org/abs/2404.13757"><code class="language-plaintext highlighter-rouge">arXiv</code></a>]</p>

  <p><span style="font-family: sans-serif"><strong>Toeplitz Low-Rank Approximation with Sublinear Query Complexity</strong></span>.<br />
  Michael Kapralov, Hannah Lawrence, Mikhail Makarov, Cameron Musco, <ins>Kshiteej Sheth</ins>.<br />
  <em>SODA 2023</em>.<br />
  [<a href="https://arxiv.org/abs/2211.11328"><code class="language-plaintext highlighter-rouge">arXiv</code></a>]</p>

  <p><span style="font-family: sans-serif"><strong>Towards Non-Uniform k-Center with Constant types of Radii</strong></span>.<br />
  Xinrui Jia, Lars Rohwedder, <ins>Kshiteej Sheth</ins>, Ola Svensson.<br />
  <em>SOSA 2022</em>.<br />
  [<a href="https://arxiv.org/abs/2110.02688"><code class="language-plaintext highlighter-rouge">arXiv</code></a>]</p>

  <p><span style="font-family: sans-serif"><strong>Fair Colorful k-Center Clustering</strong></span>.<br />
  Xinrui Jia, <ins>Kshiteej Sheth</ins>, Ola Svensson.<br />
  <em>Mathematical Programming</em>, 2021. Preliminary version in <em>IPCO</em>, 2020.<br />
  [<a href="https://arxiv.org/abs/2007.04059"><code class="language-plaintext highlighter-rouge">arXiv</code></a> | <a href="https://www.youtube.com/watch?v=E7CUukJE_9o&t=473s"><code class="language-plaintext highlighter-rouge">Talk</code></a> | <a href="https://link.springer.com/article/10.1007/s10107-021-01674-7"><code class="language-plaintext highlighter-rouge">Journal</code></a>]</p>

  <p><span style="font-family: sans-serif"><strong>Improved linear embeddings via Lagrange duality</strong></span>.<br />
  <ins>Kshiteej Sheth</ins>, Dinesh Garg, Anirban Dasgupta.<br />
  <em>Machine Learning (Springer)</em>, 2019.<br />
  [<a href="https://link.springer.com/article/10.1007/s10994-018-5729-x"><code class="language-plaintext highlighter-rouge">Paper</code></a>]</p>

  <p><span style="font-family: sans-serif"><strong>Deep-learnt classification of light curves</strong></span>.<br />
  Ashish Mahabal, <ins>Kshiteej Sheth</ins>, Fabian Gieseke, Akshay Pai, S. George Djorgovski, Andrew J. Drake, Matthew J. Graham.<br />
  <em>SSCI 2017</em>.<br />
  [<a href="https://arxiv.org/abs/1709.06257"><code class="language-plaintext highlighter-rouge">arXiv</code></a>]</p>
</div>

<h2 id="service">Service</h2>
<ul>
  <li>Conference review: ICLR, NeurIPS, ICML.</li>
</ul>
